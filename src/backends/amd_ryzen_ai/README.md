# AMD Ryzen AI Backend for xInfer

This backend provides hardware-accelerated inference on **AMD Ryzen‚Ñ¢ 7040 (Phoenix)**, **8040 (Hawk Point)**, and **Strix Point** series processors featuring the **XDNA NPU** architecture.

It leverages the **Vitis AI Execution Provider** (via ONNX Runtime) to offload matrix operations to the dedicated AI engine, freeing up the CPU and GPU.

## üöÄ Supported Hardware

*   **AMD Ryzen‚Ñ¢ 7040 Series** (e.g., Ryzen 7 7840HS)
*   **AMD Ryzen‚Ñ¢ 8040 Series** (e.g., Ryzen 7 8845HS)
*   **AMD Ryzen‚Ñ¢ AI 300 Series** (Strix Point)
*   **AMD Versal‚Ñ¢ AI Edge** (via compatible XRT drivers)

## üõ†Ô∏è Prerequisites & Installation

To use this backend, you must install the AMD Ryzen AI Software stack.

### Option A: Windows (Recommended for Laptops)

1.  **Install the IPU Driver**: 
    Ensure you have the latest AMD IPU driver installed (usually via Windows Update or AMD Adrenalin).
2.  **Install Ryzen AI Software**:
    Download and install the [Ryzen AI Software](https://ryzenai.docs.amd.com/en/latest/inst.html).
3.  **Set Environment Variables**:
    `xInfer` looks for the Ryzen AI root directory.
    ```powershell
    $env:RYZEN_AI_ROOT = "C:\Program Files\AMD\RyzenAI\1.1"
    $env:Path += ";$env:RYZEN_AI_ROOT\bin"
    ```
4.  **Install XLNX ONNX Runtime**:
    You need the specific version of `onnxruntime` packaged with Vitis AI.
    ```bash
    pip install voe onnxruntime-vitisai
    ```

### Option B: Linux (Ubuntu 22.04)

1.  **Install XDNA Driver**:
    Clone and build the [xdna-driver](https://github.com/amd/xdna-driver).
    ```bash
    git clone https://github.com/amd/xdna-driver
    cd xdna-driver && sudo ./install.sh
    ```
2.  **Install XRT (Xilinx Runtime)**:
    Download the `.deb` from the Xilinx website.
3.  **Docker (Easiest Method)**:
    It is highly recommended to build xInfer inside the `xilinx/vitis-ai-pytorch-cpu` Docker container to avoid library conflicts.

## ‚öôÔ∏è Usage

### 1. Compiling/Quantizing the Model

The Ryzen AI NPU works best with **INT8** quantization. While it supports FP32 via CPU fallback, you will not see performance gains without quantization.

Use `xinfer-cli` to quantize your model:

```bash
# This uses the underlying Vitis-AI quantizer
xinfer-cli compile \
  --target amd-ryzen-ai \
  --onnx ./models/resnet50.onnx \
  --precision int8 \
  --calibrate ./data/calib_images \
  --output ./models/resnet50_quant.onnx
```

### 2. C++ API

To run the model in your application:

```cpp
#include <xinfer/zoo/vision/detector.h>

int main() {
    xinfer::zoo::vision::DetectorConfig config;
    
    // 1. Select the Target
    config.target = xinfer::Target::AMD_RYZEN_AI;
    
    // 2. Path to the Quantized ONNX model
    config.model_path = "models/resnet50_quant.onnx";
    
    // 3. (Optional) Provide the Vitis Config JSON
    // This file is usually generated by the quantizer or found in the Ryzen AI install.
    // It tells the NPU how to split the graph.
    config.vendor_params = { "CONFIG_FILE=C:\\path\\to\\vaip_config.json" };

    // 4. Run
    xinfer::zoo::vision::Detector detector(config);
    auto results = detector.predict(image);
}
```

## üîß Troubleshooting

### "Vitis AI EP not registered"
*   Ensure `onnxruntime.dll` (Windows) or `libonnxruntime.so` (Linux) in your library path is the **AMD Vitis AI version**, not the generic Microsoft version.
*   Check if `vaip_shim.dll` is reachable in your `%PATH%`.

### "Graph falls back to CPU"
*   This usually happens if the model is not quantized. The XDNA NPU currently accelerates INT8 operators mostly.
*   Check the `xinfer` logs. If you see warnings about "Op Type not supported," re-run quantization using `xinfer-cli`.

### "NPU Hang / Timeout"
*   Ensure you are using the correct `vaip_config.json` for your specific architecture (Phoenix vs. Hawk Point).
*   Set the environment variable `XLNX_ENABLE_DUMP=1` to debug NPU instruction streams.

## üìö References
*   [Ryzen AI Software Documentation](https://ryzenai.docs.amd.com/)
*   [Vitis AI Execution Provider](https://onnxruntime.ai/docs/execution-providers/Vitis-AI-ExecutionProvider.html)
