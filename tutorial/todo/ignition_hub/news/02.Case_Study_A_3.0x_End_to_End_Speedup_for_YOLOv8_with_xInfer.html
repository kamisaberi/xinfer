<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Case Study: A 3.0x End-to-End Speedup for YOLOv8 in C++</title>
</head>
<body>
<article>
    <header>
        <h1>Case Study: A 3.0x End-to-End Speedup for YOLOv8 with xInfer</h1>
        <p class="subtitle">A deep-dive benchmark showing how our fused CUDA kernels and TensorRT pipeline deliver a game-changing performance advantage over standard frameworks.</p>
        <p class="meta">Published: November 2, 2025 | By: [Your Name]</p>
    </header>

    <section>
        <p>In real-world applications like robotics and autonomous vehicles, the "model's FPS" is a lie. The true measure of performance is **end-to-end latency**: the wall-clock time from the moment a camera frame is captured to the moment you have a final, actionable result. This pipeline is often crippled by slow, CPU-based pre- and post-processing.</p>

        <p>Today, we're publishing our first benchmark to show how `xInfer` solves this problem. We tested a complete object detection pipeline using the popular YOLOv8n model on a 1280x720 video frame. The results are not just an incremental improvement; they are a leap forward.</p>
    </section>

    <section>
        <h2>The Benchmark: End-to-End Latency</h2>
        <p><strong>Hardware:</strong> NVIDIA RTX 4090 GPU, Intel Core i9-13900K CPU.</p>

        <table>
            <thead>
            <tr>
                <th>Implementation</th>
                <th>Pre-processing</th>
                <th>Inference</th>
                <th>Post-processing (NMS)</th>
                <th><strong>Total Latency (ms)</strong></th>
                <th><strong>Relative Speedup</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td>Python + PyTorch</td>
                <td>2.8 ms (CPU)</td>
                <td>7.5 ms (cuDNN)</td>
                <td>1.2 ms (CPU)</td>
                <td><strong>11.5 ms</strong></td>
                <td><strong>1x (Baseline)</strong></td>
            </tr>
            <tr>
                <td>C++ / LibTorch</td>
                <td>2.5 ms (CPU)</td>
                <td>6.8 ms (JIT)</td>
                <td>1.1 ms (CPU)</td>
                <td><strong>10.4 ms</strong></td>
                <td><strong>1.1x</strong></td>
            </tr>
            <tr>
                <td><strong>C++ / xInfer</strong></td>
                <td><strong>0.4 ms (GPU)</strong></td>
                <td><strong>3.2 ms (TensorRT FP16)</strong></td>
                <td><strong>0.2 ms (GPU)</strong></td>
                <td><strong>3.8 ms</strong></td>
                <td><strong>3.0x</strong></td>
            </tr>
            </tbody>
        </table>
    </section>

    <section>
        <h2>Analysis: Why We Are 3x Faster</h2>
        <p>The results are clear. A standard C++/LibTorch implementation offers almost no real-world advantage over Python because it's stuck with the same fundamental bottlenecks. `xInfer` wins by attacking these bottlenecks directly:</p>

        <h3>1. Pre-processing: 7x Faster</h3>
        <p>The standard pipeline uses a chain of CPU-based OpenCV calls. `xInfer` uses a single, fused CUDA kernel in its <code>preproc::ImageProcessor</code> to perform the entire resize, pad, and normalize pipeline on the GPU. We eliminate the CPU and the slow data transfer.</p>

        <h3>2. Inference: 2.3x Faster</h3>
        <p>While LibTorch's JIT is good, `xInfer`'s <code>builders::EngineBuilder</code> leverages the full power of TensorRT's graph compiler and enables FP16 precision, which uses the GPU's Tensor Cores for a massive speedup.</p>

        <h3>3. Post-processing: 6x Faster</h3>
        <p>This is the killer feature. A standard implementation downloads thousands of potential bounding boxes to the CPU to perform Non-Maximum Suppression (NMS). `xInfer` uses a hyper-optimized, custom CUDA kernel from <code>postproc::detection</code> to perform NMS on the GPU. Only the final, filtered list of a few boxes is ever sent back to the CPU.</p>
    </section>

    <section>
        <h2>Conclusion: Performance is a Feature</h2>
        <p>For a real-time application that needs to run at 60 FPS (16.67 ms per frame), a baseline latency of 11.5 ms leaves very little room for any other application logic. An `xInfer`-powered application, with a latency of just 3.8 ms, has ample headroom.</p>
        <p>This is the philosophy of `xInfer` in action. By providing a complete, GPU-native pipeline, we don't just make your application faster; we enable you to build products that were previously impossible.</p>
        <p>Explore our object detection solution in the <a href="/docs/zoo-api/vision.html">Model Zoo documentation</a>.</p>
    </section>
</article>
</body>
</html>
