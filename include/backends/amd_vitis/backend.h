#pragma once

#include <string>
#include <vector>
#include <memory>
#include <xinfer/core/backend_interface.h>
#include <xinfer/core/tensor.h>
#include "config.h"

namespace xinfer::backends::vitis {

    /**
     * @brief AMD Vitis AI Backend
     *
     * Executes inference on Xilinx FPGAs using the VART (Vitis AI Runtime).
     * Supports loading .xmodel files generated by the Vitis AI Compiler (vai_c).
     */
    class VitisBackend : public xinfer::IBackend {
    public:
        explicit VitisBackend(const VitisConfig& config);
        ~VitisBackend() override;

        // --- Implementation of IBackend ---

        /**
         * @brief Loads the .xmodel file and initializes DPU runners.
         *
         * @note On Kria/Zynq, this assumes the DPU bitstream is loaded.
         * On Alveo, this may trigger the download of the xclbin to the card.
         */
        bool load_model(const std::string& model_path) override;

        /**
         * @brief Runs inference on the DPU.
         *
         * Handles quantization/dequantization scaling automatically if the
         * input tensors are float but the DPU expects INT8.
         */
        void predict(const std::vector<core::Tensor>& inputs,
                     std::vector<core::Tensor>& outputs) override;

        /**
         * @brief Returns device name (e.g., "Xilinx DPUCZDX8G")
         */
        std::string device_name() const override;

        /**
         * @brief Get the input scale factor for the DPU model.
         * Useful if the user wants to perform manual quantization scaling
         * in a custom kernel before calling predict().
         */
        float get_input_scale(size_t index) const;

        /**
         * @brief Get the output scale factor.
         * Useful for manual dequantization.
         */
        float get_output_scale(size_t index) const;

    private:
        // PImpl idiom to hide <vitis/ai/dpu_runner.hpp> and <xir/graph/graph.hpp>
        struct Impl;
        std::unique_ptr<Impl> m_impl;

        VitisConfig m_config;
    };

} // namespace xinfer::backends::vitis