#pragma once

#include <string>
#include <vector>
#include <memory>
#include <xinfer/core/backend_interface.h>
#include <xinfer/core/tensor.h>
#include "config.h"

namespace xinfer::backends::mediatek {

/**
 * @brief MediaTek NeuroPilot Backend
 * 
 * Executes inference on MediaTek APUs (AI Processing Units) using the 
 * Neuron Runtime.
 * 
 * Supports .pte (Pre-compiled TFLite Extension) binaries generated by
 * the NeuroPilot Converter (ncc).
 */
class MediaTekBackend : public xinfer::IBackend {
public:
    explicit MediaTekBackend(const MediaTekConfig& config);
    ~MediaTekBackend() override;

    // --- Implementation of IBackend ---

    /**
     * @brief Loads the .pte model and creates the Neuron execution context.
     * 
     * @param model_path Path to the compiled model.
     */
    bool load_model(const std::string& model_path) override;

    /**
     * @brief Executes inference on the APU.
     * 
     * Handles mapping of system memory to AHB/AXI buses for DMA efficiency
     * if supported by the kernel driver (ION/DMABUF).
     */
    void predict(const std::vector<core::Tensor>& inputs, 
                 std::vector<core::Tensor>& outputs) override;

    /**
     * @brief Returns device name (e.g., "MediaTek APU v4")
     */
    std::string device_name() const override;

    /**
     * @brief Manually sets the performance boost state.
     * Useful to trigger "Turbo Mode" only when Aegis Sky detects a target.
     * 
     * @param duration_ms How long to keep boost active (0 = forever)
     */
    void set_boost_mode(bool enable, int duration_ms = 0);

private:
    // PImpl idiom to hide Neuron Runtime headers
    struct Impl;
    std::unique_ptr<Impl> m_impl;

    MediaTekConfig m_config;
};

} // namespace xinfer::backends::mediatek